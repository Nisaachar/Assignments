{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2iSTyfLwpgDo"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# CS576 HW4, GridWorld_template.ipynb\n",
        "#\n",
        "import gym\n",
        "from gym import spaces\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class GridWorldEnv:\n",
        "    def __init__(self, size, initial_state=None, obstacles=None, goal_position=None):\n",
        "        super(GridWorldEnv, self).__init__()\n",
        "        self.size = size\n",
        "        self.state = _____                  # TODO: Set the Start at top-left corner\n",
        "        self.goal_position =   _____        # TODO: Set the goal postion\n",
        "        self.obstacles = obstacles if obstacles is not None else []\n",
        "        self.action_space = spaces.Discrete(4)  # 0: Up, 1: Down, 2: Left, 3: Right\n",
        "\n",
        "    def reset(self):\n",
        "        _____                       # TODO: Reset the start at top-left corner\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):  # 0: Up, 1: Down, 2: Left, 3: Right\n",
        "        x, y = self.state\n",
        "        # TODO: Implement the logic to update the state based on the action (0, 1, 2, 3)\n",
        "\n",
        "        _____\n",
        "\n",
        "        new_state = (x, y)\n",
        "\n",
        "        reward = _____  # TODO: Assign default reward (penalty) for each move\n",
        "        done = False\n",
        "        if new_state ________ :   # TODO: assgin the reward for reaching the goal\n",
        "            reward = _____\n",
        "            done = True\n",
        "        elif new_state ________ :   # TODO:assign the penalty for hitting an obstacle\n",
        "            reward = _____\n",
        "\n",
        "        self.state = new_state\n",
        "        return new_state, reward, done\n",
        "\n",
        "    # Grid Display\n",
        "    def render(self, start_position=(0, 0) ):\n",
        "        grid = np.zeros((self.size, self.size))\n",
        "        if self.state != start_position and start_position != self.goal_position:\n",
        "          grid[start_position] = 11  # Denote start position with 11\n",
        "        grid[self.goal_position] = 99     # Denote goal position with 99\n",
        "        for obs in self.obstacles: grid[obs] = 44   # denote obstacle position with 44\n",
        "        grid[self.state] = 7    # Denote the agent's current position with 7\n",
        "        print(grid)\n",
        "\n",
        "\n",
        "class QLearningAgent:\n",
        "\n",
        "    # n_states: The total number of unique states in the environment.\n",
        "    # n_actions: The total number of distinct actions that the agent can take in any state.\n",
        "    # size: The length of one side of the square grid\n",
        "    def __init__(self, n_states, n_actions, size, learning_rate, discount_factor, exploration_rate, max_exploration_rate=1.0, min_exploration_rate=0.01, exploration_decay_rate=0.001):\n",
        "        self.q_table = np.zeros((n_states, n_actions)) #Initialize the q table with 0.\n",
        "        self.size = size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.discount_factor = discount_factor\n",
        "        self.exploration_rate = exploration_rate\n",
        "        self.max_exploration_rate = max_exploration_rate\n",
        "        self.min_exploration_rate = min_exploration_rate\n",
        "        self.exploration_decay_rate = exploration_decay_rate\n",
        "\n",
        "    # TODO: Implement the action selection logic using exploration_rate\n",
        "    def choose_action(self, state):\n",
        "        # self.exploration_rate indicates epsilon for epsilon-greedy RL strategy\n",
        "        if ________ < self.exploration_rate: # Use random.uniform(0, 1) for exploration\n",
        "            return ________   # randomly selects an action among 0 - 3.\n",
        "        else:\n",
        "            state_index = self.grid_to_index(state)\n",
        "            # The line below finds the action with the highest Q-value for the current state.\n",
        "            # np.argmax returns the index (action in this context) of the maximum value in the given array.\n",
        "            return ________   # Exploit\n",
        "\n",
        "\n",
        "    # TODO: Implement the Q-learning update rule\n",
        "    # Hint: Update self.q_table[state_index, action] based on the reward and estimated future rewards\n",
        "    def learn(self, state, action, reward, next_state):\n",
        "        state_index = self.grid_to_index(state)\n",
        "        next_state_index = self.grid_to_index(next_state)\n",
        "\n",
        "        __________\n",
        "\n",
        "\n",
        "        self.q_table[state_index, action] = self.q_table[state_index, action] + _________________\n",
        "\n",
        "    def grid_to_index(self, position):\n",
        "        x, y = position\n",
        "        return x * self.size + y\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    # TODO: Create an instanceo of GridWorldEnv with grid size, initial state, obstcles and goal_position)\n",
        "    env = _______\n",
        "\n",
        "    # TODO: Initialize the Q-Learning agent\n",
        "    # Make sure to pass the information of the grid world environment\n",
        "    # Use the default value for max_exploration_rate=1.0,     min_exploration_rate=0.01, and   exploration_decay_rate=0.001\n",
        "    agent = _______\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    total_episodes = 10  # Run for 10 episodes\n",
        "    max_steps_per_episode = 20  # Maximum of 20 steps per episode\n",
        "\n",
        "    print(\"Start State:\", env.state, \"Goal State:\", env.goal_position)\n",
        "\n",
        "\n",
        "    # TODO: Complete ..\n",
        "    for episode in range(total_episodes):\n",
        "        state = env.reset()\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        print(f\"\\nEpisode {episode + 1}\")\n",
        "\n",
        "        for step in range(max_steps_per_episode): # Loop for a fixed number of steps or until the episode ends\n",
        "\n",
        "            action = __________________  # TODO: Agent selects an action based on its current policy\n",
        "            next_state, reward, done =  __________  # TODO: The environment responds to the chosen action with the next state, a reward, and a flag indicating if the episode is done\n",
        "            _______________  #  TODO: Call the agent's learn function - Agent updates its knowledge (like Q-values) based on the action taken and the outcome\n",
        "\n",
        "            print(f\"Step {step + 1} - State: {state}, Action: {action}, Reward: {reward}, Next State: {next_state}\")\n",
        "\n",
        "            state = next_state    # Update the current state to the next state\n",
        "            total_reward += reward   # Increment the total reward received in this episode by the reward of the current step\n",
        "\n",
        "            if done or step == max_steps_per_episode - 1:\n",
        "                print(f\"End of Episode {episode + 1}, Total Reward: {total_reward}\\n\")\n",
        "                break\n",
        "\n",
        "            if step % 5 == 0:  # TODO: Display environment every 5 steps\n",
        "                ___________\n",
        "\n",
        "        print(\"Q-table:\")   # TODO: Display Q-table every episode\n",
        "        ______________\n",
        "\n",
        "        # Update the agent's exploration rate using exponential decay, ensuring it doesn't fall below the minimum exploration rate\n",
        "        agent.exploration_rate = max(agent.min_exploration_rate, agent.exploration_rate * np.exp(-agent.exploration_decay_rate * episode))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
