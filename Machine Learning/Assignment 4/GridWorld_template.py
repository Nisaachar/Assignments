# -*- coding: utf-8 -*-
"""GridWorld_template.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IrrDNhNHGJEEXqaUyjIMGTDVUx2s4Xm7
"""

#
# CS576 HW4, GridWorld_template.ipynb
#
import gym
from gym import spaces
import numpy as np
import random

class GridWorldEnv:
    def __init__(self, size, initial_state=None, obstacles=None, goal_position=None):
        super(GridWorldEnv, self).__init__()
        self.size = size
        self.state = initial_state if initial_state is not None else (0, 0)  # Set the Start at top-left corner
        self.goal_position = goal_position if goal_position is not None else (size - 1, size - 1)  # Set the goal position
        self.obstacles = obstacles if obstacles is not None else [(1, 1), (2, 2)]  # Set two obstacles
        self.action_space = spaces.Discrete(4)  # 0: Up, 1: Down, 2: Left, 3: Right

    def reset(self):
        self.state = (0, 0)  # Reset the start at top-left corner
        return self.state

    def step(self, action):
        x, y = self.state
        if action == 0 and y < self.size - 1:  # Move Up
            y += 1
        # Implement similar checks for other directions (Down, Left, Right)

        new_state = (x, y)

        reward = -1  # Default penalty for each move
        done = False
        if new_state == self.goal_position:  # Reward for reaching the goal
            reward = 10
            done = True
        elif new_state in self.obstacles:  # Penalty for hitting an obstacle
            reward = -10

        self.state = new_state
        return new_state, reward, done



    # Grid Display
    def render(self, start_position=(0, 0) ):
        grid = np.zeros((self.size, self.size))
        if self.state != start_position and start_position != self.goal_position:
          grid[start_position] = 11  # Denote start position with 11
        grid[self.goal_position] = 99     # Denote goal position with 99
        for obs in self.obstacles: grid[obs] = 44   # denote obstacle position with 44
        grid[self.state] = 7    # Denote the agent's current position with 7
        print(grid)


class QLearningAgent:

    # n_states: The total number of unique states in the environment.
    # n_actions: The total number of distinct actions that the agent can take in any state.
    # size: The length of one side of the square grid
    def __init__(self, n_states, n_actions, size, learning_rate, discount_factor, exploration_rate, max_exploration_rate=1.0, min_exploration_rate=0.01, exploration_decay_rate=0.001):
        self.q_table = np.zeros((n_states, n_actions)) #Initialize the q table with 0.
        self.size = size
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.exploration_rate = exploration_rate
        self.max_exploration_rate = max_exploration_rate
        self.min_exploration_rate = min_exploration_rate
        self.exploration_decay_rate = exploration_decay_rate

    # TODO: Implement the action selection logic using exploration_rate
    def choose_action(self, state):
        if random.uniform(0, 1) < self.exploration_rate:
            return random.randint(0, 3)  # Random action
        else:
            state_index = self.grid_to_index(state)
            return np.argmax(self.q_table[state_index])


    # TODO: Implement the Q-learning update rule
    # Hint: Update self.q_table[state_index, action] based on the reward and estimated future rewards
    def learn(self, state, action, reward, next_state):
        state_index = self.grid_to_index(state)
        next_state_index = self.grid_to_index(next_state)

        max_next_action = np.argmax(self.q_table[next_state_index])
        current_q = self.q_table[state_index, action]
        next_q = self.q_table[next_state_index, max_next_action]

        new_q = (1 - self.learning_rate) * current_q + self.learning_rate * (reward + self.discount_factor * next_q)
        self.q_table[state_index, action] = new_q

    def grid_to_index(self, position):
        x, y = position
        return x * self.size + y


def main():
    env = GridWorldEnv(size=4)
    n_states = 16  # Total number of unique states in the environment (4x4 grid)
    n_actions = 4  # Total number of distinct actions

    agent = QLearningAgent(
        n_states, n_actions, size=4,
        learning_rate=0.1, discount_factor=0.9, exploration_rate=1.0
    )

    # Rest of the code remains unchanged...





    total_episodes = 10  # Run for 10 episodes
    max_steps_per_episode = 20  # Maximum of 20 steps per episode

    print("Start State:", env.state, "Goal State:", env.goal_position)


    # TODO: Complete ..
    for episode in range(total_episodes):
        state = env.reset()
        total_reward = 0
        done = False

        print(f"\nEpisode {episode + 1}")

        for step in range(max_steps_per_episode):
            action = agent.choose_action(state)
            next_state, reward, done = env.step(action)
            agent.learn(state, action, reward, next_state)

            print(f"Step {step + 1} - State: {state}, Action: {action}, Reward: {reward}, Next State: {next_state}")

            state = next_state
            total_reward += reward

            if done or step == max_steps_per_episode - 1:
                print(f"End of Episode {episode + 1}, Total Reward: {total_reward}\n")
                break

            if step % 5 == 0:
                env.render(start_position=(0, 0))

        if (episode + 1) % 5 == 0:  # Display Q-table after every 5 episodes
            print("Q-table:")
            print(agent.q_table)


if __name__ == "__main__":
    main()